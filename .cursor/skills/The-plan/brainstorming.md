
video transcript:

If you're still using AI in the browser, you're doing it the slow way. You see each of these apps has a terminal version and they make me 10 times
faster. I'm getting so much work done and the AI companies are kind of quiet about this. They're marketing these tools to developers for code,
but here's what they're not telling you. You can use them for everything and it's way better than their apps.
Writing research projects, working in the terminal is a superpower. I'm about to show you why I'm literally writing this video with these tools
right now and most people have no idea this is a thing, but I'm telling you, once you see AI in the terminal, you're never going back to the browser,
so get your copy ready. We're going to my favorite place, the terminal, but hold up before we go there. I know what you're thinking, Chuck.
I use AI just fine. Do you tell me if this sounds like you because this is how I used to use ai.
You're in the browser app, you're asking questions, research mode. You're diving deep into a project, can't even see your scroll bar anymore,
and this is your fifth chat because chat GBT lost its context or it's mine. You also created a few more chats with Claude and Gemini to make sure chat GBT
wasn't lying and yeah, you tried to copy and paste some stuff into your notes app to keep track That never works. At this point, your project is a mess. Spread over 20 chats,
two deep research sessions and scattered notes. There's a better way to do this. Hear me out, attend the terminal.
Now we're not going to waste any time we're diving straight into the terminal and I know you probably have some questions. Put those in your pocket for a second and we'll address those,
Segment 1: The Terminal Superpower (Gemini)
but I first just want to show you what it looks like. I want you to try it so you can know it's not really scary. The terminal's a fun place and we're going to play with Gemini CLI first. Why?
Because it has a very generous free tier. That's right. You heard it free and I'll show you claw code in a second.
That's my favorite by far Overpowered, but they share a lot of the same concepts. Let's just get you going. We can install it with one command now go ahead and launch a terminal.
It doesn't really matter where you launch it. Mac, windows, Linux, all these terminal apps work everywhere. Great for me.
I'm going to use Windows with WSL or the Windows subsystem for Linux. If you have no idea what that is, it's totally fine. I got a video right here,
so I'll launch my Ubuntu terminal, zoom in so you guys can see, and we'll copy and paste this command installing Google Gemini CLI. Ready, set,
go Coffee break. While that goes, never checked our coffee and if you run into a scary issue,
run it with pseudo just like this and if you're on Mac, you can also use Brew Brew. Install Gemini dash CLI. Now it's installed.
Before we launch it, we're going to make a new directory, M-K-D-I-R. Then we'll call it coffee project just like this and let's jump in there.
CV Coffee Project. Now we can launch Gemini. You'll see why I did this here in a second. Type in Gemini. One word. Ready,
set, go. We're here now first, isn't that logo just awesome? I love the terminal. It's so nostalgic.
Now first thing you'll do is get logged in with your Google account. Everyone has a Google account and yes, this can be a free regular Gmail account.
This can open your browser. Sign in and I'm logged in. Now don't be scared. Go ahead, ask it a question like how do I make the best cup of
coffee in the world? I love the little messages that give us herding digital cats. It's doing a Google search right now, pretty sick,
crafting the guide and then it responds just like we're used to. That wasn't so bad was it? But notice some superpowered things. First of all,
we got Gemini two five Pro, the latest and greatest model. Also, the browser doesn't show you this 99% context left.
Every chat you have with AI has a context window. The browser hides it from you and the terminal does not. Also, your browser can't do this watch.
I really want you to find the best way to make coffee research the top 10 sites, only reputable sources,
and then compile the results into a document named Best Coffee method.md and then create me a blog plan, just an outline. I'll do the writing ready,
set and go. Alright, now it's asking us a question. Do you want me to write a file for you? Do you want me to create a file for you?
Yeah, dude, go for it. This thing can do everything a browser can do, but it has a superpower. It can access your computer,
it can read and write files like I'm not copying and pasting this. It's doing it for me. I mean look, it actually made files on our computer.
There they are. Best Coffee Method coffee blog. Think about that for a second. It can access your obsidian vault,
all your notes because those are just files sitting there on your hard drive. It can run Bash and Python scripts. It can do mostly everything because we broke it out of the browser. Anyways,
back to the terminal. If we type in forward slash tools and hit enter, you can see all that Gemini's allowed to do. You can even add more tools,
but this feature right here is what made me switch from the browser to the terminal. Watch this type in forward slash and it just like that go,
what it's doing right now is something powerful. It's creating a Gemini MD file and in the process it analyzed our project,
read our folder, read our files and yes, go ahead buddy. Create that file for me.
What it just did there was create instructions for itself, context for what we're working on. Let's take a look at it.
Cat gemini.md and while we didn't do much in this project, it knows, it knows what's going on and every time you launch Gemini,
it's going to load that file as its context like let's test it. So we still have our Gemini session open.
I'll open up another Gemini session in that same directory. This is a new conversation, fresh context, a hundred percent left notice.
It's using our new gemini.md file and I'll tell it this. Write the intro for blog post one and the coffee series.
No more context, just that it should know exactly what I'm talking about. Look at that. Yep, go ahead buddy. I didn't give it any context,
it just knew this is a new chat session and as I work, I can just ask Gemini to update that file with my thoughts, research decisions.
We made the progress of our project and look, it added a summary to what we were working on to our gemini.md file.
I can close all this. Start up a new session. It picks up where we left off. No re-explaining the context, no starting over, no more 20 scattered chats.
We just had this one file that helps keep us organized. Everything you need, you're never paralyzed again. Now when I saw this, I'm like, this is it.
I finally have control over my context, my files, my projects. They're not stuck in some browser chat session anymore.
They're right here sitting on my hard drive mine, my precious.
Now I'm literally using the gemini.md file right now for this project. This is a small example. Let me show you a big one.
This is the gemini.md file for this video. It tracks everything. It describes how I create project status, major decisions I made,
even other documents should look at. Yeah, it can do that. I'll launch a whole new Gemini session in this project and I'll ask you for
where we're at in the project. Are you seeing this? This has completely changed the way I create or do anything now and I don't just
use Gemini. It's not even close to the best one. Let's look at CLO code, my daily driver. This one's so crazy. Now,
real quick time for a coffee break and a bit of a PSA thanks to our sponsor, twin Gate. Now I'm teaching you and millions of other people how to use AI and the
terminal, which will also give AI access to your computer and all your files and access to do pretty much anything. This is cool, but also really scary.
Especially scary if they're a remote employee because not only does your remote employee or just remote person that you give access to your lab or your internal
network have access to everything over a traditional VPN if that's what you're still using. I'm talking your servers, file servers, nas,
because that's normally how traditional VPNs work. If someone has remote access to the network, their computer has access to everything in the network,
which means their AI may have access to everything in your network. This is insane and it's why traditional VPNs are dead or at least should be.
You should be using Twin Gate instead. Twin Gate is all about zero trust network access, meaning they zero trust people connecting to your network and you should zero
trust them too. With Twin Gate, your employee is given access to nothing unless you explicitly give it.
So Beatrice here, she's on the marketing team and given access to the nas, but only the NAS and nothing else and definitely not access to the super secret
file server where we keep all our coffee recipes. No, no, no, no, no, no. And Twin Gate takes this to the next level.
This is one of my favorite features right here. Trusted profiles. You can make it to where not just any device can connect to your network and
it'll do checks to see like, Hey, are they running Android? Sorry buddy, you're out of here because iPhones are clearly better.
Everyone agrees or if they have Windows we can make sure a hard drive encryption is required and they have screen lock and a firewall and antivirus and their OS
is up to date and we can block Windows 10. All these checks will happen before they're allowed to connect to your network.
This is crazy because it's also free for up to five users. If you're a business, it's a no-brainer, but for like a home laber, you seeing this,
you can geek out with some enterprise security. It's just fun. And speaking of enterprises, if I create a trusted Windows profile,
I can connect this to my CrowdStrike, Intune, Jamf one, whatever EDR or EDMI have and have that tell me if that device is secure or if
it meets my standards. Now, me and my team, we use Twin Gate every single day. If you're still using traditional VPN, you're crazy, try out Twin Gate.
You can set it up in about five minutes and it seriously is the best, most secure way to access your home lab or your business network remotely.
So go check it out, twin gate.com/network chuck or check the link below, thanks to Twin Gate for sponsoring this video and this channel.
They help make my videos possible. I love my partnership with them. Anyways, coffee break over back to the video. Now I use cloud code,
Segment 2: Claude Code
which is Claude in the terminal for pretty much everything. It's my default and here's why. It has a feature that changes the game agents.
Look at this. I have seven agents performing tasks right now in one terminal. Actually there's 10, and listen,
that's just one of the seven features it has that keeps me glued to the terminal. Now, cloud code is not free, but I do have good news.
If you already pay for Claude Pro, which starts at like 20 bucks a month, you can log into the terminal with this subscription and use it. So yeah,
you don't have to use API keys. And by the way, if you can only pay for one AI subscription, cloud Pro is the one I would choose,
especially for the last feature I'm going to show you. Now, let's get it installed with one command. We'll copy and paste this bad boy right here, NPM install. You get it now?
I already have it installed. It should tell me that or ask me to do pseudo it yelled at me again, I'm sorry, and then we'll launch Claude very similarly to Gemini.
Just type in Claude and your directory. That's it. It'll prompt you to get logged in and then ask permission to access this folder.
Yes, of course. No. Again, don't be afraid. Ask Claude a question. I need to find a NAS for my house. Here's my budget. What I want to do,
create a report for me and just like Jim and I, it does have some fun little messages for us. It'll search the web.
It's got this nice TUI terminal user interface. Oh, whoops. Our thinking is off. I can press tab to turn on thinking mode.
Now we're thinking and notice it is security first. It does ask you permission to do most things and that's good.
We can get past that here in a moment if you're feeling dangerous. I am, so we respond similar to Gemini. It can also have a context file too.
If I run that same command slash knit, it will create what's called a Claude MD file,
noticing a trend here and it's doing the same thing, looking through our project, trying to figure out what's what and creating a context file.
And also similar to Gemini, we can do something like forward slash context, which is so crazy. We can see exactly how many tokens we've used,
how much we have left and exactly what's using it. But what's funny with Claude,
that doesn't really matter too much as long as you know how to use their most powerful feature agents, let's make a Claude agent right now.
It's really simple. We'll do four slash agents just like this. We'll get a terminal menu and let's create a home lab research expert and I'll
just tell it that. So create a new agent. Notice it'll ask us where do you want to make it? Because you can have agents that are tied to just this project we're working on
or personal agents that are tied to everything. You can always call them. We'll do just this project. We'll use Claude to make it.
We can tell it what we want it to be. It's going to paste this in here and let it go to work. Notice you can give it access to tools or restrict access.
We'll give it everything. Choose our model sonnets, great auto color. There's our agent.
Press enter to save and we'll hit escape to get out of that interface and we just made our first agent home lab guru, but what's the point of that?
I kind of feel like we can just ask Claude to do research for us because we literally just did that. You're right, but watch this.
I'm going to do a four slash context once more so we can get our baseline. We've used 85,000 tokens. I'll give you this prompt and I'm calling it home lab agents.
It'll figure it out. I'll have a create a document and I'll say, make sure you reference the research we made.
I can just do an at symbol and look at the documents in our directory is the NAS rec folder. There we go. Watch. It's going to use the home lab guru agent.
There it is. There's our agent researching for us right now. Now here's why this is amazing, actually kind of insane. So Claude was like,
cool, I've got a task but it's not for me. I'm going to delegate this task to one of my employees or one of my coworkers,
and this is another Claude instance. It's like a guy sitting over there. He's like, Hey buddy, are you busy? Here's some work to do.
He's giving him a fresh set of instructions and get this a fresh context window. You saw just now we have 200,000 tokens in our context window.
We use 42% of it. This guy, he's got a fresh 200. That means the conversation we're having right now, me and the main CLA guy,
it's protected. It doesn't get too bloated. I can give tasks to other sub-agents and never have to leave this conversation.
Claude just delegated this task to a new agent. He's got a fresh pot of coffee, he's ready to go.
He just walked into work and you can create a ton of different agents for different purposes. I use them all the time to protect my context and avoid any kind of weird bias.
For example, here's my YouTube outline for this video. I'll normally be working with Claude on an outline like this and often the
context window can get pretty big. Like this is where I'm at right now and let's say at this point I want a comprehensive review of my outline. I have a brutal critic agent.
I can just deploy. Hey, let's have, I'll just do at and call him. There he is, agent, brutal critic, review my outline. And by the way, this is a fun one.
It's designed to just completely roast me according to the framework I have and the scripting goals and it just deploys it. My conversation is still sane,
but I have a new energized employee, just got a new sub coffee in hand, fresh eyes ready to go. Look at him,
go and you can run multiple agents like I showed you at the beginning of this segment. Let's go check in our home lab agent. Actually, he's been kind of busy.
Oh, he's still going. We're going to have the best home lab ever and by the way, we can hit control O to see what he's doing. Oh no, I interrupted him.
I interrupted. Sorry. Keep going. Don't mind me. Actually, let's open up a new window, jump into our project,
which we still have coffee project. It's okay, it'll figure it out. Launch Claude in there. He's got his Claude MD file.
He knows what's going on and let's run this prompt. We'll launch our home lab agent to research the best proximate servers.
At the same time, do a general agent that'll just search for the best pizza place in Dallas, Texas, and then another home lab agent that'll find the best graphics card for gaming
right now and then put it all together in a comprehensive report. Let's go. Now that's doing its stuff right now, deploying new employees over here.
I've got the home lab guru working, finding the best network equipment. I feel so powerful right now. This is so fun. Yeah, they're the three agents go.
Of course we got to give it permission to search the web and stuff a lot. Actually, I'll show you how to get past that here in a moment. Actually,
I'll show you right now. I'm going to actually interrupt them. I'm going to hit control C to get out of there. Control C twice.
We can run this switch and actually I'll do Claude dash R so I can resume a previous conversation. Yes, you can do that. And then I'll do dash,
dash dangerously, dash skip permissions. This is cloud without training wheels. Ready, set, go.
We can choose that previous chat we had. Let's do this one here. Yes, I interrupted you. I'm sorry. Go ahead and do it again. And notice right here,
bypass permissions on go. Now we're cooking, man. Look at this. Three employees, fresh cups of coffee just going.
I told you the terminal was awesome. Let's get crazier. I'll create another tab because no one can stop us to our project Launch cloud
in dangerous mode. And what do you see? We have Claude, use Gemini for research. Yes,
because we can run Gemini and Claude and all these terminal tools in headless mode, meaning you don't jump into a TUI, you just run them with one command.
It's like Gemini dash P and then here's your prompt. Find pizza. We can just create an agent that does that. Let's make it happen.
Create a new agent. I'll give it a basic prompt. You are a research expert.
You will use Gemini in headless mode to research. Use it like this. That should do it.
Let's check in our home lab guru. Yeah, he finished up. It's looking back at our home lab guru. He used 40,000 tokens just himself,
fresh employee. Let's check our context for this conversation. We only used 4,000 more tokens for that. It was 85, now we're at 88. Alright,
now let's find the best AI terminal videos on YouTube. Use the Gemini research agent
and look at that. It's using Gemini to search. We're having an AI use in AI right now. There's the prompt to want to hit control O. Let's check on our multi-agent,
see how they're doing. They're still working man. Alright, it finished up. Our agent used Gemini to find the top 10 videos,
but this next feature is tied with how awesome agents is and I'm embarrassed to say I just found this out while making this video. Now, cloud code,
it was built for coders. It's in the name, but I hacked it kind of. Cloud code has a feature called output styles.
Look at this word slash output style. We got a few defaults in here and these essentially control the system prompt
for claw code, how it's going to respond to you. The persona it has, the default is code and that's what we're using, but we can change that.
Let's create a new one. So I'll go back here, type an output style again and do output style new. It's actually an agent.
I didn't know that. Let's just be simple. You are a home lab expert designed to help me
create the best home lab possible. That's it. Now we can get a lot crazier than that, but you get the idea and that's it.
If I type in output style, there it is, I can adopt that right now or when I back out and I jump back in.
I think it should adopt that style by default. Yes it does. You can create output styles that are tied to just your project or as a whole,
whatever project you jump into, you can switch to those output styles. I'm using the output style right now to make this video. You want to see it script writing, output style and this is what it looks like.
It's pretty intense. Optimize for what I do. Now, honestly, this entire video could be about clo.
There are so many features like you can do shift tab to cycle between modes. One of my favorites is planning mode, hit shift tab. There were plan mode on
this will put a very well thought out plan together and then you approve it and then it just does it like there it is and no keep planning or yes,
let's keep doing it. And there are so many things I can't dive into 'em. They have prompts, hooks, custom status lines.
You can paste images into your terminal. But here's the thing, I don't just use Claude code, I use Gemini. I use Claude code and I use Codex,
Segment 3: ALL THE TERMINALS!!!
which is Chad Biz's terminal tool all at the same time. Let me show you how Gemini, Claude Code, Chad, BTS, codex.
I'm using all three right now to work on this video script. How two steps. First, as long as I open up Claude, Gemini and Codex in the same directory,
they're all using the same context. It's the same project. Let's do that right now. Claude Gemini,
if I spell it right and Codex, the second thing I do is I make sure my context files are all synced up.
They all say the same thing. So Gemini, do MD, claw MD and Agents md, which is what Codex uses and they're trying to make it a standard.
They're all the same. And I usually have a terminal open for each one while I'm working on a script or any kind of project. Watch this,
I'll tell Claude to write a hook for this video authority angle. Write it to Authority Hook md.
I'll have Jim and I write a hook on a discovery angle, write it to Discovery hook.md and then I'll have Codex
review it. And that's normally what I do. I find chat. BTS very good at analyzing things from a high view.
Gemini and Claude are very good at the work, the deep work and look at it go. They're all using the same context, different roles.
I mean have three different ais working on the same thing at the same time. No copying and pasting. They can see each other's work.
They're working in the same directory. That's awesome. And it's done. Looks like Claude won this one. Good job, Claude.
Now are you seeing what's happening here? This is the craziest part about this. Everything I'm doing talking with these three different ais on a project,
it's not tied in a browser, it's not tied in a gui. It's just this folder right here on my hard drive.
I can copy and paste that folder anywhere. All the work, all the decisions, all the context. It's mine. And that's the difference.
Nothing annoys me more than when Chad GBT tries to fence me in. Give me that vendor Lockin. So I can't leave. No, I reject that.
I own my context. If a new greater, better AI comes out, I'm ready for it because all my stuff is right here on my hard drive.
I will use all ai, I'll use the best ai. No one could stop me. So I got a little excited there and that's what I want you to take away here.
Leading the browser, going to your terminal, puts you back in control and it gives you better features.
But now I want to get real practical. I want to show you exactly how I run a project like this. How I keep things in sync, how I keep my claw,
Gemini and agent's files in sync and work on a daily basis using a system like this. I'm going to refill my coffee. I'll see you there. Okay.
Segment 4: Meta-Proof
This video was made with this process. First thing I want to show you is how things are synced up. Specifically my Claude file, my Gemini file, and my agent's file,
which is Codex. It's an agent. That's all it is. So if I get back to my terminal here where I was showing you earlier when we had
our hooks developed, Claude made one, made one and Codex critiqued it. I rely on Claude to run my agent. That will close out everything.
So I'll just normally do this when I'm done for the day. I'll go, Hey, let's close this out,
run and I'll mention my agent script session closer. Lemme show you what that looks like.
This is one of those agents I keep as a personal agent. I use it for many projects and these agents are just files.
They're files inside an agent's folder. I'll cat that file here, script session closer and here it is. It does a lot of stuff,
but some of the key things it does, and actually while I'm talking with you about this, I'm going to go and run the agent. We did some stuff, didn't we?
I'll run the agent, but first it'll gather everything we talked about, everything we did and do a comprehensive summary.
It'll then update a session summary file that's specific to just updating what are some things that were done in the past sessions.
It'll see if any core project files need to be updated. And if I'm talking with Claude, it'll update every context. File Claude Gemini agents and then this is probably my favorite part.
I commit my project to a GitHub repo. So normally you would use Git for code for writing and deploying code.
I treat my scripts and pretty much every project I work on in my life like code, we commit that change,
give a reason for that change so I can see a history of what I did and why I did it. Maybe something breaks, I can go back to that change and reinstate it.
That's the power of using GitHub with all your ideas. Now this is killer for me because I'm really bad at documentation.
I'm really bad at keeping track of things, but now I have this help me keep track of things. So when I'm really tired at the end of the day and I've been working on a video
and my mind is fried, I'm like, okay, tell you what, I'm done. Close this out. It will look through all this stuff. It will figure out where I'm at,
end the project, end the day where I need to be and then I can start fresh the next day and be
like, Hey, where are we at? What are we working on? They can tell me, Hey Chuck, you finished the script. It's time to record. We made these three decisions,
go for it. That's it. This is the tool I've been looking for.
The syncing is probably my favorite part. No, no, actually this thing's my favorite part. How it roasts me.
So I don't only use these AI terminal tools to help me create. I use them to critique me and make me better. So here are my critics.
These guys are so stinking mean and I designed them to be they're agents. I get the brutal critic. I told 'em to be mean.
So I had an issue where my AI was being way too agreeable. I'd write something and be like, oh Chuck, best thing you ever wrote. I'm like,
ah, you're gaslighting me. Stop it. I wanted something to be super mean. I want it to be hard to please so that when it did tell me I did a good job,
I knew it. It was good and that's what this thing does. So actually I'm curious if my script right now is any good.
I actually have not used it yet on this, which I normally don't do. It's bad. Let's see. I'll say use the brutal critic to absolutely
roast and verify my stuff is good and this script, I'll just reference my script again,
everything's a file local here on my file system. I mean you can just move so quickly and nimbly when you have a system like this
and the agent's going to go, it's going to do its thing. And again, what I love about this is while I've been talking with this AI for a minute,
right? We've been going back and forth doing things. If I asked that session to review me, it would have a ton of bias coming into that based on what we've been talking
about. I don't want that. I want a fresh cup of coffee credit coming in, going, here's what I know network Chuck needs to have and I'm going to roast his
current script on what he's doing. That's what I need. Notice it was really quick editor free stream on that.
I have inside its instructions to read some framework documents I have that describe how I create and describe what my thing is, who my audience is. Again,
I don't copy and paste. I got context already here. You can reference that. I love this so much. It's my favorite thing. I had to make a video about this.
I wouldn't stop telling my wife about it and she was going crazy. Whenever he talks to me, it just goes in one ear and out the other.
I tell everyone about this. I'm actually really scared about what it's going to say. Oh no. Okay,
let's see what it said, said when I fix it, it'll be good. What did it tell me? Oh, it's an 8.2. It's not bad. Now I forgot I did this.
The brutal critic has three personalities or three people that come in and roast it from different angles. I forgot I did that. It's insane. Okay,
I don't like this. I'm not sure I agree with this. We'll see. It says segment five is a feature dump nine features, zero depth,
five to 8% retention drop, which means you'll stop watching. Let's prove it wrong. Segment five five's coming up.
We're currently in segment four. Jump there if you want to. I don't care. Tell me if it's bad. It's actually really cool I hope,
but seriously doing stuff like this saves me hours. Now also, I want you to know this. It's not writing for me.
I'm doing the writing because I like to keep that. I think that's important now with ai, but I do have AI roast me,
help me stay on track. I get distracted. I might make some really bad creative decisions that aren't in line with what
we're trying to do. Now, the point I want to hit home is that I made this for me. This is my own personal software, exactly my use case.
What can you build for you that's just for you and your niche and whatever you're trying to make happen and this is just one tool I've made.
Look at the other stuff I've done. That crazy project I showed you in the beginning was running like seven agents.
This is a custom built project just with claw code and I'm going to use it right after I record this. It's going to go through every file,
it's going to transcribe it, read it, and give my editors notes. They need to be able to edit this video like,
Hey Chuck repeated himself 17 times in the fourth video and it's a disaster. Ignore that one. It will do that. Seriously,
the tools I create are so powerful for me. I wake up every day feeling like I have superpowers. I want this for you. Now,
Segment 5: Open Code
speaking of superpowers, let's talk about segment five. There's a tool that's actually open source.
Now you can use any model you want with this open source alternative and it might be the best tool of all of them. I'm still testing it.
You also get grok free, which is pretty sick and a really powerful part of this is you can log in with your cloud Pro subscription and use it like cloud code. Let's play with it.
It's called open code. We can install it with one command. I'll have all these commands below installing. That was quick and that's it.
If I can just launch open code, just type it in like this. That does it. Oh, let me do a source because it edits to my bash RC file.
You might have to do this too. Source bash rc. Now let's try it. You can open and close your terminal and do the same thing.
Oh, we're here. This is open code. A nice two E terminal user interface. Couple of things real quick. They launched this straight into grok code Fast one.
They have a deal with grok AI that allows you to use this for free for a while, so same story. Hey, GR code's helping us even knows like,
Hey, can I help you with your coffee project? No mask stuff. So it read the document and it can do all the stuff. Okay, cool.
I'm going to exit out. We can use local models. This is the killer part. I don't think any other tool does this,
but we have to edit a file talking about nano and I think it's inside config Open code. Yeah,
that's the directory and then I think the file has to be open code JSON C. I'll paste this in here. I'm going to use,
let's do llama 3.2. I think that's what have installed. We'll find out. I'll save that. Let see llama list, see what my models are.
Do I have 3.2? I'll pull it down if I don't, and then when I launch OPA code again with that config in place, oh wait,
what did I break? Lemme fix that file real quick. There we go. I should be able to go forward slash model and change to LAMA 3.2.
Hey, cool Lama works. If I back out, I can log into cloud with this command,
open code auth login. I can choose Aaro with my cloud pro and I'll launch this browser,
paste this code in there and now log into cloud code. I just launch open code now and switch models to cloud
sonet 4.5 and we can pick up our wheel left off because again, all our files are local. It doesn't matter what tool we use.
It's all ours right here. I can switch models midway. Let's go back to gro.
Cool, what's our next step? Cool. While it's doing that, I can do new session. This is a new session.
There's models to Claude. I can do sessions, see all the sessions I have. Why do I have so many? Oh,
these are my past sessions. We can also share these sessions slash share and watch this.
This is so crazy. I think I was going to do it. Oh, the URL is copied in my clipboard, so I can go to my browser,
paste that URL in and I can, I mean, look at that. That's awesome. Wait, is it live? You can share your session with people. That's pretty neat.
Okay, we can do timeline. We can jump back in time and restore.
I guess if we like how far we've come in our conversation, we just go back. I want that in real life. That's awesome. We can also,
if I jump out of there and do open code help, they've got agents. You can start a headless server.
You can attach to headless servers. Export your session is JSO data. I mean this tool is pretty cool and the fact that you can log in and use your
cloud pro subscription, that's next level because otherwise you're putting in an API key and you're
paying per use and that's a whole nightmare. I'd rather pay up front. What's fun is I've been following these guys on Twitter before they started
making this code, open code, this guy Dax, these guys are killing it. So how do you feel about your browser based gooey AI now? Pretty bad, right?
Kind of feels like hammer and chisel or chisel and hammer. I never know how to say it because now you can control your context. Break out of that browser,
##########################################################################################################################################################################
Video summary and analysi of topics and his /agents and methods
##########################################################################################################################################################################
This video argues that using AI in the terminal is significantly faster and more powerful than using browser-based AI tools (0:00). The presenter demonstrates Gemini CLI, Claude Code, and Open Code, highlighting how these command-line interfaces provide better control over context, files, and projects.

The video focuses heavily on the concept of "agents" within Claude Code (8:43), which are described as a game-changing feature for multi-tasking and maintaining clear context.

Here's a breakdown of the agents discussed:

    In-depth discussion of a specific agent:
        Home Lab Research Expert/Guru (10:46): The presenter creates this agent in detail, demonstrating how it can be given a specific purpose (to research home lab equipment) and how it receives a fresh context window when delegated a task (12:08). This allows the main conversation with Claude to remain uncluttered.
    Agents mentioned and their purposes:
        Brutal Critic Agent (12:56): This agent is designed to provide harsh and unbiased reviews of outlines or scripts, ensuring the work is genuinely good rather than simply agreeable. The presenter later shows this agent in action (23:02).
        Script Session Closer Agent (20:59): This personal agent (used across multiple projects) automatically summarizes conversations, updates project files (like the claude.md, gemini.md, and agents.md context files), and commits changes to a GitHub repository at the end of a work session (21:13). This helps with documentation and tracking project history.
        General Agent (13:55): This is briefly mentioned as an example when delegating a simple search task (finding the best pizza place).
        Research Expert (using Gemini) (15:27): The presenter explains how a Claude agent can be created to utilize Gemini in headless mode for research, showcasing inter-AI delegation.
    Agents shown on screen (without detailed discussion):
        At the beginning of the Claude Code segment, the presenter briefly shows a screen with multiple agents performing tasks simultaneously (8:51), illustrating the multi-tasking capability. He states there are "seven agents performing tasks... Actually, there's 10" (8:51-8:54).
        During the demonstration of running multiple agents concurrently, the Home Lab Guru and a General Agent are shown actively working (13:50-14:15).
        The Brutal Critic Agent's output and score for the video script are shown on screen (24:49-25:15).

do the same for commmand he uses and and contaxt savong, token saving stragis he uses

The video highlights several commands and strategies for context and token saving, emphasizing that these methods allow for a more efficient and organized workflow outside of browser-based AI.
Commands Used:

The speaker demonstrates or mentions several commands throughout the video for installing and interacting with the AI tools:

    Installation Commands:
        pip install google-generativeai (1:51): Used to install Google Gemini CLI.
        sudo pip install google-generativeai (2:00): An alternative for when the initial install faces permission issues.
        brew install gemini-CLI (2:03): For installing Gemini CLI on Mac using Homebrew.
        npm install -g @anthropic-ai/claude-cli (9:18): Used to install Claude Code.
    Directory Management:
        mkdir coffee project (2:10): Creates a new directory.
        cd coffee project (2:13): Changes the current directory to the new project.
    Launching AI Tools:
        gemini (2:18): Launches the Gemini CLI tool.
        claude (9:28): Launches the Claude Code tool.
        opencode (26:56): Launches the Open Code tool.
    AI Interaction/Special Commands within Tools:
        for/tools (4:00): Shows what Gemini is allowed to do.
        for/init (4:10): Used in both Gemini and Claude to initialize and create context files (gemini.md, claude.md).
        cat gemini.md (4:31): Displays the content of the gemini.md file.
        for/context (10:23): Shows token usage and context information in Claude.
        for/agents (10:41): Opens a terminal menu to create and manage Claude agents.
        @agent_name (11:41, 13:01): Used to call or deploy a specific Claude agent within a conversation.
        Ctrl + O (13:30): Used to check on the progress of a running Claude agent.
        Ctrl + C (14:26): Used to interrupt an AI's process.
        claude -r (14:31): Resumes a previous Claude conversation.
        claude --dangerously-skip-permissions (14:36): Launches Claude without requiring permission for every action.
        gemini -P "prompt" (15:20): Runs Gemini in headless mode with a direct prompt.
        for/output_style (16:03): Manages and applies output styles in Claude.
        shift + tab (17:37): Cycles between different modes in Claude, such as "planning mode" (17:39).
        opencode o login (28:27): Logs into Claude Pro via Open Code.
        for/model (28:18): Changes the active AI model in Open Code.
        for/share (29:16): Shares a session in Open Code via a URL.
        for/timeline (29:36): Allows users to jump back in time within a session.
        opencode --help (29:48): Displays help information for Open Code.

Context Saving and Token Saving Strategies:

The speaker emphasizes that the core benefit of terminal AI tools is their ability to own and manage context locally, overcoming the limitations of browser-based AI.

    Project-Aware Context Files (.md files):
        Automated Context Creation: Gemini, Claude, and Open Code all use an init command (or similar functionality) to create .md files (e.g., gemini.md, claude.md, agents.md) within the project directory (4:10, 10:10). These files automatically analyze the project, read existing files, and generate instructions or context for the AI.
        Persistent Context: Every time the AI tool is launched in that directory, it automatically loads this context file (4:38). This eliminates the need to re-explain the project or problem, preventing the "20 scattered chats" issue (5:19).
        Dynamic Updates: The AI can be instructed to update these context files with thoughts, research, decisions, and project progress, ensuring the context always reflects the current state of the work (5:07).
        Local Ownership: All context and project files reside on the user's hard drive, preventing vendor lock-in and allowing easy transfer or use with different AI tools (19:37, 20:01).
    Agent Delegation (Claude Code's Token Saving Strategy):
        Fresh Context Windows: Claude's "Agents" feature is the primary method for managing and effectively "saving" tokens (11:53). When a task is delegated to an agent (which is essentially another Claude instance), that agent receives a fresh, full context window (e.g., 200,000 tokens) (12:12).
        Protected Main Conversation: This means the main conversation with Claude doesn't get "bloated" or "too big" (12:26), as the heavy lifting and token consumption for specific sub-tasks are offloaded to the agent. This prevents the main context from being filled and improves efficiency by giving the agent a clean slate for its task.
        Specialized Roles: Agents can be given specific "personalities" or roles (e.g., "brutal critic," "home lab expert") (10:46, 23:02). This specialization helps maintain focus and prevents bias in the main conversation.
    Multi-AI Syncing:
        The speaker runs Gemini, Claude Code, and Codeex (ChatGPT's terminal tool) in the same directory (18:15).
        He ensures all their respective context files (gemini.md, claude.md, agents.md) are synced up and contain the same information (18:30). This allows multiple AIs to work on the same project with shared context but different roles (e.g., ChatGPT for analysis, Gemini/Claude for deep work), without copying and pasting (19:12).
    Git Integration for Project History:
        The "script session closer" agent automatically commits the project to a GitHub repository (21:43). This treats the project (even writing scripts) like code, providing a historical record of changes, decisions, and documentation, allowing the user to track progress and revert if necessary (21:57:55). This eliminates the need for manual documentation at the end of a tired day (22:11).

##########################################################################################################################################################################

##########################################################################################################################################################################

Integrating a Custom OpenAI Assistant with
Home Assistant (Long-Term Memory Enabled)
Limitations of the Official OpenAI Conversation Integration
Home Assistant’s built-in OpenAI Conversation integration is convenient, but it has no built-in long-term
memory or support for custom prompt IDs. Each query is essentially stateless – the assistant doesn’t
automatically remember past interactions, and you can’t directly use a saved prompt from your OpenAI
project. The official integration simply sends your query (plus some basic instructions/template) to the
OpenAI API and returns the answer. It doesn’t log conversations to your OpenAI account or maintain
context between questions. To achieve persistent memory and use your custom assistant (with prompt ID
pmpt_... ), we’ll need a more custom setup.
Using OpenAI’s Responses API (Replacing the Assistants API)
OpenAI introduced an Assistants API (for creating persistent assistants with tools and memory), but that
API is now deprecated in favor of the new Responses API . The Responses API is the recommended
way to interact with your custom assistant. Essentially, you will:
Use your saved prompt (with its prompt ID) in API calls. This ensures the assistant uses the
instructions and personality you defined on the OpenAI platform. In the API request, you include a
prompt_id field referencing your custom prompt . OpenAI will then use the latest version of
that prompt plus any variables or context you pass in the call.
Allow conversation logging and memory on OpenAI’s side. By using the Responses API (and
enabling conversation storage), each turn can be stored in OpenAI’s system, meaning you can review
them in your OpenAI dashboard logs and maintain context across turns. (We’ll cover how to preserve
context shortly.)
Why not use the old Assistants API? Since it’s being phased out by 2026, it’s better to use the new
approach. The Responses API combined with conversation management gives you the same benefits
(custom instructions, tools, memory) in a future-proof way.
Setting Up Home Assistant’s MCP Server for Full Control
To let your OpenAI assistant actually control Home Assistant (turn devices on/off, read sensors, etc.), the
best method is to use Home Assistant’s Model Context Protocol (MCP) Server integration. This essentially
turns Home Assistant into a tool/API that an AI assistant can use. Here’s how to set it up:
Install/Enable the MCP Server integration in Home Assistant. In Settings > Devices & Services >
Add Integration, search for “Model Context Protocol Server” and add it. This will expose an endpoint
(usually https://<your-ha-url>/api/mcp ) that conforms to OpenAI’s tool API spec.
1
•
2
•
1.
1
Allow control and expose entities: During setup or in the MCP Server integration options, make
sure “Control Home Assistant” is enabled (so the AI can execute commands) . Then go to the
Exposed Entities page (Home Assistant Assist settings) and check all the entities/devices you want
the AI to access . Since you plan to start with everything exposed, you might bulk-expose all
entities initially. (Be cautious: this literally gives the AI the ability to turn on/off or change any device
state that’s exposed.) You can refine this list later once you see what the assistant actually uses.
Authentication: Decide how the OpenAI side will authenticate to your Home Assistant MCP server.
The simplest way is to create a Long-Lived Access Token in Home Assistant (go to your User Profile
> Long-Lived Access Tokens) and copy that string. This token will be used by OpenAI as a bearer
token to authorize the MCP requests. (Home Assistant’s MCP supports OAuth as well, but a long-lived
token is easiest for a custom setup.) Copy your HA URL and this token for the next steps.
At this point, Home Assistant is ready to be controlled by an external LLM client via MCP. Essentially, HA will
present the AI with a set of “tools” (one for each exposed device or service) that it can call (this happens
behind the scenes). For example, a light might be a tool the AI can invoke to turn it on/off. Home Assistant’s
Assist API is used under the hood for these actions.
Connecting Your OpenAI Assistant to Home Assistant (via MCP)
With both your custom OpenAI prompt and Home Assistant’s MCP endpoint in hand, you can now “connect”
them using the OpenAI Responses API. The idea is to include Home Assistant as an MCP tool in your API
call. OpenAI’s platform allows models to use connectors and remote MCP servers as tools in a
conversation . In our case, Home Assistant will be a remote MCP server tool. Here’s how to do it:
When you call the POST /v1/responses API (OpenAI Responses endpoint), include a tools
array in the JSON payload. This array should contain an entry for your Home Assistant MCP server.
For example:
"tools": [
{
"type": "mcp",
"server_label": "HomeAssistant",
"server_description": "Home Assistant server for smart home control",
"server_url": "https://YOUR_HA_URL/api/mcp",
"authorization": "Bearer YOUR_LONG_LIVED_TOKEN",
"require_approval": "never"
}
]
Let’s break down these fields:
- type: "mcp" indicates a custom MCP server (as opposed to a built-in connector).
- server_url: the URL to your HA MCP endpoint. Use your external URL (or internal, if your OpenAI calls will
reach it) and make sure to include the /api/mcp path. This is the streaming SSE endpoint that the
OpenAI model will connect to.
- authorization: the token for auth. OpenAI will pass this as an Authorization header when connecting. In
our example, we prefixed it with "Bearer " since Home Assistant expects a bearer token in the header. (If
2.
3
4
3.
5
•
2
you run into auth issues, ensure the format matches what HA expects. Usually "Bearer <token>" is
correct for long-lived tokens.)
- server_label/description: A friendly label and description. These aren’t strictly required, but it’s good
practice. The model might use the description to understand what the tool is (“Home Assistant server for
smart home control”).
- require_approval: set to "never" to allow the AI to call Home Assistant’s tools autonomously without
pausing for developer approval . (If you prefer to manually approve each action, you could use
"always" , but that’s not practical for a voice assistant scenario.)
Include your prompt ID in the request. The request JSON can have either an input with messages
and a system prompt, or a prompt object referencing a saved prompt. In your case, use:
"prompt": {
"prompt_id": "pmpt_697049f9....",
"variables": { ... }
}
This tells OpenAI to use the prompt you created on their platform (with all your custom instructions) . If
your prompt has any variables defined, supply them in the variables map. If not, you can omit
variables or leave it empty.
Specify the model. Use a model that supports tools (function calling and connectors). For example,
gpt-4 (or specifically the gpt-4o variant if available to you) would be suitable, since the
gpt-4o family supports OpenAI’s tool/connector system. The official HA integration by default uses
gpt-4o-mini , but you can choose a more powerful model if your API access allows. Ensure the
model you request is one that supports the Responses API and tool use. (GPT-3.5 from before Nov
2023 likely won’t support the new tools, whereas GPT-4 and beyond do .)
Provide the user’s query as the input . This can be a simple string for a single-turn question, or an
array of messages if you want to manually include roles. For instance: "input": "Turn on the
living room lights and set the temperature to 72."
Putting it all together, the JSON payload for a single request might look like:
{
"model": "gpt-4o",
"prompt": { "prompt_id": "pmpt_697049f9...123" },
"tools": [
{
"type": "mcp",
"server_label": "HomeAssistant",
"server_url": "https://your-ha-url/api/mcp",
"authorization": "Bearer ABCDEFGH...12345",
"require_approval": "never"
}
6
5 7
•
2
•
8 9
•
3
],
"conversation": "conv_0123456789abcdef",
"input": "Turn on all the kitchen lights and play some music."
}
Let’s walk through what happens when this is called: - OpenAI will initialize the conversation using your
custom prompt (so the assistant has whatever personality/knowledge you gave it). - The model sees your
user input and also notices it has a tool (HomeAssistant) available. It will connect to your HA MCP
endpoint to retrieve the list of available actions/tools (HA will expose your devices and services as tool
definitions) . This tool listing is automatic – you don’t have to script it; the integration handles it. (For
example, HA might advertise a tool like light.turn_on with certain parameters, etc. All exposed entities
and services become potential tools.) - The model will decide if it needs to call any of those tools to fulfill the
request. In this example, “turn on all the kitchen lights” clearly maps to calling the relevant light control tool
in HA. It will format a tool call (via MCP) to Home Assistant. The HA MCP server receives that, executes it
(turning on the lights), and returns the result (success or data back). - The model then continues generating
a response. It might say “OK, I’ve turned on the kitchen lights and started playing music.” The output from
the OpenAI API will include both the assistant’s reply text and records of any tool calls it made. The
resp.output_text (in OpenAI’s response object) will contain the final answer to speak to you . In
the raw response JSON, you’ll also see items of type mcp_call showing details of each tool usage .
You generally don’t need to do anything with those mcp_call logs except perhaps for debugging or
audit. What you care about is the assistant’s spoken reply and the real-world actions executed in HA.
Important: Because we set "require_approval": "never" , the AI’s tool calls will execute immediately.
If you were to set it to "always" , the API would pause and return an approval_request_id waiting
for you (the developer) to approve the tool call – not suitable for a home assistant scenario, so keep it at
never for hands-free operation .
Maintaining Long-Term Conversation State
To give your assistant long-term memory, you should leverage the Conversation features of the OpenAI
API. There are two ways to persist context between queries:
Using Conversation IDs: You can create a conversation object via the Conversations API and supply
its ID in each Responses API call. When you include a "conversation": "conv_<ID>" field in the
request, all exchanges will be tied to that conversation on OpenAI’s side, preserving state across
sessions and devices . This is ideal for your case – you could create one conversation for your
Home Assistant assistant and keep using it indefinitely. All messages, including tool calls, become
part of that conversation’s history. OpenAI does not purge conversation data after 30 days (unlike
single response logs) if it’s attached to a conversation object . That means your assistant can truly
have long-term memory.
How to use it: After the first API call, you’ll get a conversation_id (if you didn’t supply one, OpenAI will
create a new conversation automatically if you set store: true ). You can also explicitly create one via
openai.conversations.create() beforehand . Save that ID (e.g., in a variable or Home Assistant
secret). On subsequent calls, include "conversation": "<that ID>" in the JSON. The model will
10 11
12 13
14 15
5 10
•
16
17
18
4
retrieve prior messages from that conversation for context on each new request, without you needing to
resend the entire history every time.
Using Previous Response Chaining: Alternatively, the API offers a simpler (but less durable) way to
continue a chat by passing a previous_response_id . If you set "store": true on each call,
OpenAI will keep a 30-day record of that response. You can then call the next response with
"previous_response_id": <last_response_id> to continue the thread . This
implicitly carries over the conversation. However, this approach is a bit less straightforward to
manage across many turns and the data may expire after 30 days (since it’s not tied to a permanent
conversation object). For a truly long-running assistant, the conversation ID method above is
recommended.
Recommendation: Use the Conversations API approach for a home assistant with long-term memory. For
example, create a conversation once (perhaps manually via a small script or the first time the user invokes
the assistant), and always reuse that conv_... ID for future queries to your assistant. This will ensure the
AI remembers prior Q&A, device states it told you about, your preferences, etc. across days and weeks.
According to OpenAI, conversation objects are durable and not subject to the 30-day expiry that individual
responses have .
Also, by default, the Responses API calls with store:true will log each turn in your OpenAI project Logs
(visible on the OpenAI dashboard) for 30 days . If you attach them to a conversation, they’ll also be
retained via the conversation. This meets your requirement of “logging on my OpenAI platform” – you’ll be
able to review what your Home Assistant assistant said or did from the OpenAI side. (Just be mindful of
privacy: those logs reside on OpenAI’s servers. Avoid sharing sensitive info unless you’re comfortable with it being
stored there under your account.)
Implementing the Solution in Home Assistant’s Assist Pipeline
With the above pieces in place, you have a capable OpenAI agent that can converse and act on Home
Assistant. The final step is wiring this into Home Assistant’s Assist interface (so that you can speak or
type to it via Home Assistant and get responses back).
There are a couple of ways to do this:
Custom Conversation Agent Integration: The cleanest approach is to create or use a custom
conversation agent that calls the OpenAI Responses API as outlined. For example, the popular
Extended OpenAI Conversation custom component allows multiple custom agents with different
prompts. You could potentially configure one of those to use your prompt (though it may not yet
support the new Responses API features out of the box – you’d need to verify). Otherwise, writing a
small custom integration that implements the ConversationAgent interface and calls the OpenAI
API is an option if you’re comfortable with Python. This would let you select your custom assistant in
Home Assistant’s Voice Assistant settings, just like you select the built-in OpenAI or local
conversation agent.
Home Assistant Automation + REST Call: If you want a quicker (if a bit hacky) solution, you can use
Home Assistant’s automation or scripting to act as the go-between. For example, you could set up an
intent script or automation that triggers whenever you use Assist (there’s an event or intent you can
•
19 20
17
17
•
•
5
catch when a conversation is started). That automation could then make a RESTful call to the
OpenAI API with the user’s query and retrieve the answer, then speak it out. Concretely, you might
use the rest_command integration or even a curl command via a shell_command to POST to
https://api.openai.com/v1/responses with the JSON payload (including prompt_id,
conversation, tools, etc.). The response will come back as JSON; you’d extract the output_text and
then use Home Assistant’s TTS or notification to deliver it to the user. This method requires some
careful templating and scripting, but it avoids writing a full custom component. Keep in mind you’ll
need to parse JSON in a template or use a small Python Script/Node-RED to handle the response
parsing.
Regardless of method, the flow is: User query (voice or text) -> Home Assistant passes it to OpenAI ->
OpenAI returns the answer (and possibly executes actions via MCP) -> Home Assistant outputs the
answer (voice reply or text). From the user’s perspective, it will feel like the regular Assist feature – but
now backed by your powerful custom GPT assistant with memory and full device control.
One more tip: If you go the custom route, you might initiate the conversation ID once and store it in Home
Assistant (in a secret or input_text). This way, every time you call the API, you reuse that ID. If the
conversation ever gets too lengthy or context window overflows, you can start a fresh one and perhaps
archive the old. But typically GPT-4 can handle a large amount of context, so it should be fine for ongoing
use, especially if you occasionally summarize or prune context via your prompt design.
Post-Setup: Refining Permissions and Behavior
After you’ve got everything working end-to-end, monitor how the assistant behaves with full access. You
mentioned giving it “complete control” initially – you will want to watch the logs (both Home Assistant logs
and OpenAI tool call logs) to see what it’s doing. If it’s overzealous or using tools you don’t want it to, you
should go back to Home Assistant’s exposed entities list and start restricting what it can see and do. For
example, you might expose only certain domains (lights, switches, climate) and not others (perhaps alarm
systems or door locks) depending on your comfort level. Home Assistant’s MCP integration lets you limit
which entities are exposed as tools, so use that to sandbox the AI as needed.
Similarly, you can refine your OpenAI prompt over time. Since you have a saved prompt on the platform,
you can update its instructions if you find the assistant needs guidance (for example, “Don’t unlock doors
after 10 PM” or “If unsure about a command, ask for confirmation.”). You can update the prompt version in
the OpenAI Playground and then use the new prompt ID or version number in your API calls . The
beauty of using the prompt ID is you don’t have to hard-code the whole system prompt in Home Assistant –
you can tweak it on OpenAI’s side and just deploy the new version.
Finally, keep an eye on API usage and costs. The kind of integration we’ve built here (especially with GPT-4
and tools) can be quite heavy in token usage. Every device state exposed, every tool call, and the
conversation history count toward tokens. OpenAI’s logging will help you see how many tokens each
conversation turn uses. You might want to set some safeguards (in OpenAI’s dashboard, you can set
monthly cost limits ). Also, consider using a slightly lower-powered model for casual queries if cost is a
concern – e.g., maybe use gpt-3.5-turbo-1106 for simple chit-chat and only use GPT-4 when you need
complex reasoning or tool use. (That said, tool use might only be supported on the newest models, so test
accordingly.)
21
22
6
Conclusion
In summary, to integrate your custom OpenAI assistant with Home Assistant’s Assist feature:
Use the OpenAI Responses API with your saved prompt ID (instead of the limited built-in
integration) so you can leverage your custom instructions and have the conversation logged on the
OpenAI side.
Set up Home Assistant’s MCP server and expose your entities, which gives the AI real control over
your smart home via a standardized interface .
Connect the two by including Home Assistant as an MCP tool in your OpenAI API calls . The AI
will then be able to call Home Assistant’s Assist API functions as needed to perform actions (lights,
thermostats, etc.).
Implement conversation memory by using the Conversations API or previous response IDs, so
that each query builds on the last and the AI retains context long-term . This fulfills the “long
term memory” requirement and keeps logs in your OpenAI dashboard for review.
Integrate into Home Assistant’s UI either via a custom conversation agent or an automation that
handles sending queries to OpenAI and relaying responses back to the user. This makes the
experience seamless – you can talk to your home just like the default assist, but with a far more
capable brain in the background.
Refine and secure: After confirming it works, gradually restrict the AI’s permissions to only what you
need , and adjust the prompt or model parameters to ensure reliability and safety.
By following this approach, you’ll effectively have a personalized AI butler for your smart home – one that
remembers your past interactions and can autonomously manage your devices through natural
conversation. Good luck, and enjoy your new AI-enhanced Home Assistant setup!
Sources:
Home Assistant MCP Server documentation (Home Assistant allows LLM clients to control exposed
devices via the Assist API) .
OpenAI Prompt and Responses API documentation (using saved prompt IDs in API calls) , and
support for connectors/MCP tools in API requests .
OpenAI Conversation State documentation (maintaining conversation context across API calls with
conversation IDs) .
OpenAI API Reference (Assistants API deprecation notice, recommending the Responses API) .
Home Assistant OpenAI integration docs (for model options and setup basics) .
OpenAI Tools and Connectors guide (explains how models can use remote MCP servers as tools with
proper authorization) .
Assistants | OpenAI API Reference
https://platform.openai.com/docs/api-reference/assistants
Prompting | OpenAI API
https://platform.openai.com/docs/guides/prompting
Model Context Protocol Server - Home Assistant
https://www.home-assistant.io/integrations/mcp_server/
•
•
4
• 6
•
16
•
•
23
•
24
• 2
6
•
16
• 1
• 8 9
•
5
1
2 21
3 4 23 24
7
Connectors and MCP servers | OpenAI API
https://platform.openai.com/docs/guides/tools-connectors-mcp
OpenAI - Home Assistant
https://www.home-assistant.io/integrations/openai_conversation/
Conversation state | OpenAI API
https://platform.openai.com/docs/guides/conversation-state
5 6 7 10 11 12 13 14 15
8 9 22
16 17 18 19 20
8